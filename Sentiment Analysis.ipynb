{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_COLUMNS = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "dataset = pd.read_csv('twitter_sentiment_data.csv', encoding=\"ISO-8859-1\", names=DATASET_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop(columns=[\"ids\", \"date\", \"flag\", \"user\"])\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_map = {0: \"NEGATIVE\", 2: \"NEUTRAL\", 4: \"POSITIVE\"}\n",
    "def decode_sentiment(label):\n",
    "    return decode_map[int(label)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "dataset.target = dataset.target.apply(lambda x: decode_sentiment(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.groupby(['target']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "target_cnt = Counter(dataset.target)\n",
    "\n",
    "plt.bar(target_cnt.keys(), target_cnt.values())\n",
    "plt.title(\"Dataset labels distribuition\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not necessary after first run\n",
    "\n",
    "import re\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "\n",
    "\n",
    "def preprocess(text, stem=False):\n",
    "    # Remove link,user and special characters\n",
    "    negations = re.sub(\"n't\", \"not\", str(text).lower())\n",
    "    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n",
    "    \n",
    "    tokens = []\n",
    "    for token in text.split():\n",
    "        if token not in stop_words:\n",
    "            if stem:\n",
    "                tokens.append(stemmer.stem(token))\n",
    "            else:\n",
    "                tokens.append(token)\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.text = dataset.text.apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_tweets = dataset[dataset['target']==\"NEGATIVE\"]\n",
    "pos_tweets = dataset[dataset['target']==\"POSITIVE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(neg_tweets, pos_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = neg_tweets.text.str.split(expand=True).stack().value_counts()\n",
    "pos = pos_tweets.text.str.split(expand=True).stack().value_counts()\n",
    "\n",
    "values_neg = neg.keys().tolist()\n",
    "counts_neg = neg.tolist()\n",
    "\n",
    "values_pos = pos.keys().tolist()\n",
    "counts_pos = pos.tolist()\n",
    "\n",
    "plt.bar(values_neg[0:10], counts_neg[0:10])\n",
    "plt.title('Top 10 Negative Words')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.bar(values_pos[0:10], counts_pos[0:10])\n",
    "plt.title('Top 10 Positive Words')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words='english', binary=False, ngram_range=(1,1))\n",
    "\n",
    "neg_cv = cv.fit_transform(neg_tweets['text'].tolist())\n",
    "pos_cv = cv.fit_transform(pos_tweets['text'].tolist())\n",
    "\n",
    "freqs_neg = zip(cv.get_feature_names(), neg_cv.sum(axis=0).tolist()[0])\n",
    "freqs_pos = zip(cv.get_feature_names(), pos_cv.sum(axis=0).tolist()[0])\n",
    "\n",
    "list_freq_neg = list(freqs_neg)\n",
    "list_freq_pos = list(freqs_pos)\n",
    "\n",
    "\n",
    "list_freq_neg.sort(key=lambda tup: tup[1], reverse=True)\n",
    "list_freq_pos.sort(key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "cv_words_neg = [i[0] for i in list_freq_neg]\n",
    "cv_counts_neg = [i[1] for i in list_freq_neg]\n",
    "\n",
    "cv_words_pos = [i[0] for i in list_freq_pos]\n",
    "cv_counts_pos = [i[1] for i in list_freq_pos]\n",
    "\n",
    "plt.bar(cv_words_neg[0:10], cv_counts_neg[0:10])\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.title('Top Negative Words With CountVectorizer')\n",
    "plt.show()\n",
    "\n",
    "plt.bar(cv_words_pos[0:10], cv_counts_pos[0:10])\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.title('Top Positive Words With CountVectorizer')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tv = TfidfTransformer(stop_words='english', binary=False, ngram_range=(1,3))\n",
    "\n",
    "neg_tv = tv.fit_transform(neg_tweets['text'].tolist())\n",
    "pos_tv = tv.fit_transform(pos_tweets['text'].tolist())\n",
    "\n",
    "freqs_neg_tv = zip(tv.get_feature_names(), neg_tv.sum(axis=0).tolist()[0])\n",
    "freqs_pos_tv = zip(tv.get_feature_names(), pos_tv.sum(axis=0).tolist()[0])\n",
    "list_freq_neg_tv = list(freqs_neg_tv)\n",
    "list_freq_pos_tv = list(freqs_pos_tv)\n",
    "list_freq_neg_tv.sort(key=lambda tup: tup[1], reverse=True)\n",
    "list_freq_pos_tv.sort(key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "cv_words_neg_tv = [i[0] for i in list_freq_neg_tv]\n",
    "cv_counts_neg_tv = [i[1] for i in list_freq_neg_tv]\n",
    "\n",
    "cv_words_pos_tv = [i[0] for i in list_freq_pos_tv]\n",
    "cv_counts_pos_tv = [i[1] for i in list_freq_pos_tv]\n",
    "plt.bar(cv_words_neg_tv[0:10], cv_counts_neg_tv[0:10])\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.title('Top Negative Words With tf-idf')\n",
    "plt.show()\n",
    "\n",
    "plt.bar(cv_words_pos_tv[0:10], cv_counts_pos_tv[0:10])\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.title('Top Positive Words with tf-idf')\n",
    "plt.show()\n",
    "\n",
    "x = dataset['text']\n",
    "y = dataset['target']\n",
    "\n",
    "cv = CountVectorizer(stop_words='english', binary=False, ngram_range=(1,3))\n",
    "x_cv = cv.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TRAIN_SIZE = 0.7\n",
    "\n",
    "x_train_cv, x_test_cv, y_train_cv, y_test_cv = train_test_split(x_cv, y, test_size=TRAIN_SIZE, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(x_train_cv,y_train_cv)\n",
    "\n",
    "y_pred_cv = model.predict(x_test_cv)\n",
    "print(confusion_matrix(y_test_cv,y_pred_cv))\n",
    "print(classification_report(y_test_cv,y_pred_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
